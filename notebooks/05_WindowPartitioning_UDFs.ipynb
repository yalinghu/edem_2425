{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNkZfzfxGZ0z"
      },
      "source": [
        "# Windows Partitioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQieQ5pkGfNm"
      },
      "source": [
        "## Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HelxRmCPGpql"
      },
      "source": [
        "Install Spark and Java in VM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9Cn3c-ywGtDV"
      },
      "outputs": [],
      "source": [
        "# install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# download spark 3.5.3\n",
        "!wget -q !wget /q https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D95sNcJfGvyV",
        "outputId": "6c16b8ee-027c-42ff-b9a7-025862e295fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 391476\n",
            "drwxr-xr-x 1 root root      4096 Nov 25 19:13 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r-- 1 root root 400864419 Sep  9 05:35 spark-3.5.3-bin-hadoop3.tgz\n"
          ]
        }
      ],
      "source": [
        "ls -l # check the .tgz is there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qtBMGi7EGvwN"
      },
      "outputs": [],
      "source": [
        "# unzip it\n",
        "!tar xf spark-3.5.3-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6JO331NrGvtt"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02epIDkbG24d"
      },
      "source": [
        "Defining the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qmON5zHJG4-m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgvNJQOAGZ00"
      },
      "source": [
        "Start Spark Session\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "siaPZq4XGZ00",
        "outputId": "ef21e20b-2635-40fc-a119-a3c38bf4571d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-3.5.3-bin-hadoop3\")# SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Window Partitioning\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "nsBkpLh6GZ01",
        "outputId": "1b20d7fe-0257-48be-bfe6-a489e200f141"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e02755507f0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://10f9fc3daa6a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Window Partitioning</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Bqu4fQnNGZ02"
      },
      "outputs": [],
      "source": [
        "# For Pandas conversion optimization\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-9DDmYQKGZ02"
      },
      "outputs": [],
      "source": [
        "# Import sql functions\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYrtXWZIHKMt"
      },
      "source": [
        "Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2lkKBm3CHL-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394ba15b-4761-4f34-e796-c1263de363d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "characters.csv\tdeptmanagers.csv  employees.csv  salaries.csv  titles.csv\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p dataset\n",
        "!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/characters.csv -P /dataset\n",
        "!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/employees.csv -P /dataset\n",
        "!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/salaries.csv -P /dataset\n",
        "!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/deptmanagers.csv -P /dataset\n",
        "!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/titles.csv -P /dataset\n",
        "!ls /dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxWVtHu5GZ02"
      },
      "source": [
        "## Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vDZRclR1bz2"
      },
      "source": [
        "### Window Partitioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JWgnuYZ8GZ02"
      },
      "outputs": [],
      "source": [
        "employeesDF = spark.read.option(\"header\", \"true\").csv(\"/dataset/employees.csv\")\n",
        "salariesDF = spark.read.option(\"header\", \"true\").csv(\"/dataset/salaries.csv\")\n",
        "deptManagersDF = spark.read.option(\"header\", \"true\").csv(\"/dataset/deptmanagers.csv\")\n",
        "titlesDF = spark.read.option(\"header\", \"true\").csv(\"/dataset/titles.csv\")\n",
        "charactersDF = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/dataset/characters.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m26N92p-GZ02",
        "outputId": "c277c9d6-24cc-480e-aa84-11c7ae597b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+----------+\n",
            "|emp_no|   title| from_date|   to_date|\n",
            "+------+--------+----------+----------+\n",
            "| 10010|Engineer|1996-11-24|9999-01-01|\n",
            "| 10020|Engineer|1997-12-30|9999-01-01|\n",
            "+------+--------+----------+----------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "titlesDF.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9KiG0PHGZ03"
      },
      "source": [
        "Get the last title for the employees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W89HFrvGZ03",
        "outputId": "bc8785ce-7406-45b4-9680-d592f0e23531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+----------+----------+\n",
            "|emp_no|          title| from_date|   to_date|\n",
            "+------+---------------+----------+----------+\n",
            "| 10040|       Engineer|1993-02-14|1999-02-14|\n",
            "| 10040|Senior Engineer|1999-02-14|9999-01-01|\n",
            "+------+---------------+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "titlesDF.filter(col(\"emp_no\") == 10040).show(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vJzQSe5pGZ03"
      },
      "outputs": [],
      "source": [
        "# import library\n",
        "from pyspark.sql.window import Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smIYb3ogGZ03",
        "outputId": "348085cb-7482-4add-8c6a-aba33429e358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+----------+\n",
            "|emp_no|          title|   to_date|\n",
            "+------+---------------+----------+\n",
            "| 10040|Senior Engineer|9999-01-01|\n",
            "+------+---------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get last title by date for each employee (similar to previous exercise in Joins)\n",
        "byEmployee = Window.partitionBy(\"emp_no\").orderBy(col(\"to_date\").desc())\n",
        "mostRecentJobTitlesDF = titlesDF.withColumn(\"datesOrder\", row_number().over(byEmployee)) \\\n",
        "    .filter(col(\"datesOrder\") == 1) \\\n",
        "    .select(\"emp_no\", \"title\", \"to_date\")\n",
        "\n",
        "mostRecentJobTitlesDF.filter(col(\"emp_no\") == 10040).show(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fcF-z4WGZ03"
      },
      "outputs": [],
      "source": [
        "# for the previous example we saw in joins, if we use windows partitioning we do not need to do the previous step (filtering max date and then do the join over the same table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2uP3Mo-GZ04"
      },
      "source": [
        "Get the max salaries (three for example) for job title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUTvHq4EGZ04",
        "outputId": "c401687c-e283-4100-a248-44be5a221476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+\n",
            "|salary|          title|\n",
            "+------+---------------+\n",
            "| 72668|Senior Engineer|\n",
            "+------+---------------+\n",
            "\n",
            "+------+---------------+\n",
            "|salary|          title|\n",
            "+------+---------------+\n",
            "| 80324|       Engineer|\n",
            "| 47017|       Engineer|\n",
            "| 88806|Senior Engineer|\n",
            "+------+---------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# we need salary data with title data joined first\n",
        "bestPaidPerTitlerawDF = salariesDF.withColumn(\"salary\", col(\"salary\").cast(\"long\")) \\\n",
        "    .join(mostRecentJobTitlesDF, (salariesDF.emp_no == mostRecentJobTitlesDF.emp_no) & (salariesDF.to_date == mostRecentJobTitlesDF.to_date)).drop(\"emp_no\", \"from_date\", \"to_date\")\n",
        "\n",
        "bestPaidPerTitlerawDF.filter(salariesDF.emp_no == 10040).show()\n",
        "bestPaidPerTitlerawDF.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7-8h-0nGZ04",
        "outputId": "95be0223-c104-4e88-fe88-0f3c04865583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------+-----------+\n",
            "|salary|             title|rank_salary|\n",
            "+------+------------------+-----------+\n",
            "|101622|Assistant Engineer|          1|\n",
            "| 92674|Assistant Engineer|          2|\n",
            "| 92034|Assistant Engineer|          3|\n",
            "|130939|          Engineer|          1|\n",
            "|121819|          Engineer|          2|\n",
            "|120417|          Engineer|          3|\n",
            "+------+------------------+-----------+\n",
            "only showing top 6 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# now we apply the window partitioning\n",
        "byTitle = Window.partitionBy(\"title\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "bestPaidPerTitleDF = bestPaidPerTitlerawDF.withColumn(\"rank_salary\", row_number().over(byTitle)).filter(col(\"rank_salary\") <= 3)\n",
        "\n",
        "bestPaidPerTitleDF.show(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFCL2_6U1bz4"
      },
      "source": [
        "### UDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpG24BxT1bz4",
        "outputId": "525f131a-9cb9-450f-b71a-17ac832bddfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------+----+----------+-----------+---------+----------+------+---------+-------+\n",
            "|          name|height|mass|hair_color| skin_color|eye_color|birth_year|gender|homeworld|species|\n",
            "+--------------+------+----+----------+-----------+---------+----------+------+---------+-------+\n",
            "|Luke Skywalker|   172|  77|     blond|       fair|     blue|     19BBY|  male| Tatooine|  Human|\n",
            "|         C-3PO|   167|  75|        NA|       gold|   yellow|    112BBY|    NA| Tatooine|  Droid|\n",
            "|         R2-D2|    96|  32|        NA|white, blue|      red|     33BBY|    NA|    Naboo|  Droid|\n",
            "+--------------+------+----+----------+-----------+---------+----------+------+---------+-------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "charactersDF.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atBTFBlY1bz5",
        "outputId": "f78aa836-68e1-4a06-ef52-ebaf9d3f95bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------+----+----------+-----------+---------+----------+------+---------+-------+----+\n",
            "|          name|height|mass|hair_color| skin_color|eye_color|birth_year|gender|homeworld|species|year|\n",
            "+--------------+------+----+----------+-----------+---------+----------+------+---------+-------+----+\n",
            "|Luke Skywalker|   172|  77|     blond|       fair|     blue|     19BBY|  male| Tatooine|  Human|  19|\n",
            "|         C-3PO|   167|  75|        NA|       gold|   yellow|    112BBY|    NA| Tatooine|  Droid| 112|\n",
            "|         R2-D2|    96|  32|        NA|white, blue|      red|     33BBY|    NA|    Naboo|  Droid|  33|\n",
            "+--------------+------+----+----------+-----------+---------+----------+------+---------+-------+----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# we define a function to remove the BBY from the birth_year\n",
        "year = udf(lambda s: s[:-3])\n",
        "\n",
        "charactersDF.withColumn(\"year\", year(col(\"birth_year\"))).show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYWyYjXPGZ04"
      },
      "source": [
        "## Exercises Window Partitioning\n",
        "1) Load characters.csv to a DataFrame. Then, get the (two) tallest characters per species and per homeworld planet. Select the name, height, and species/homeworld in their case.\n",
        "2) Get the height difference for each character with respect to the smallest one in the same homeworld."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndRe6jVQGZ04"
      },
      "source": [
        "Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uz7mRJwGZ05"
      },
      "source": [
        "Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtHxnyJc1bz5"
      },
      "source": [
        "## Exercises UDFs\n",
        "1. Choose one of the DFs we have, define two UDFs of your own, and apply them (with a withColumn) on the DF. Show the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdA3Q2K3GZ05"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ff1af5cda0bea4fe5c4ebc1f94ab9f13d8998f98d08e16d8aba48673b9d00116"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}